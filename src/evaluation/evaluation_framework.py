"""
Evaluation Framework for irAE Detection System

This module provides systematic evaluation of the irAE detection system
with proper metrics for production deployment.

AI Expert Approved: Comprehensive metrics for clinical AI evaluation.
"""

import json
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import asyncio

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.assessment import Severity, Urgency, Likelihood, OrganSystem
from src.models.patient import PatientData
from src.llm.assessment_engine import IRAEAssessmentEngine
from src.parsers import LabParser, MedicationParser, SymptomParser


@dataclass
class ExpectedOutcome:
    """Ground truth for a test case."""
    irae_detected: bool
    severity: Severity
    urgency: Urgency
    affected_systems: List[OrganSystem]
    likelihood: Likelihood
    
    # Tolerance settings
    severity_tolerance: int = 1  # Allow ¬±1 grade
    

@dataclass
class EvaluationResult:
    """Result of evaluating a single case."""
    case_id: str
    
    # Predictions
    predicted_irae: bool
    predicted_severity: Severity
    predicted_urgency: Urgency
    predicted_systems: List[OrganSystem]
    predicted_likelihood: Likelihood
    
    # Ground truth
    expected: ExpectedOutcome
    
    # Correctness
    irae_correct: bool = False
    severity_exact: bool = False
    severity_within_tolerance: bool = False
    urgency_correct: bool = False
    systems_precision: float = 0.0
    systems_recall: float = 0.0
    systems_f1: float = 0.0
    likelihood_correct: bool = False
    
    # Timing
    inference_time_ms: float = 0.0
    
    def calculate_metrics(self):
        """Calculate all metrics for this result."""
        # irAE detection
        self.irae_correct = self.predicted_irae == self.expected.irae_detected
        
        # Severity
        self.severity_exact = self.predicted_severity == self.expected.severity
        pred_rank = _severity_rank(self.predicted_severity)
        exp_rank = _severity_rank(self.expected.severity)
        self.severity_within_tolerance = abs(pred_rank - exp_rank) <= self.expected.severity_tolerance
        
        # Urgency
        self.urgency_correct = self.predicted_urgency == self.expected.urgency
        
        # Organ systems (precision/recall/F1)
        pred_set = set(self.predicted_systems)
        exp_set = set(self.expected.affected_systems)
        
        if len(pred_set) > 0:
            self.systems_precision = len(pred_set & exp_set) / len(pred_set)
        else:
            self.systems_precision = 1.0 if len(exp_set) == 0 else 0.0
            
        if len(exp_set) > 0:
            self.systems_recall = len(pred_set & exp_set) / len(exp_set)
        else:
            self.systems_recall = 1.0 if len(pred_set) == 0 else 0.0
            
        if self.systems_precision + self.systems_recall > 0:
            self.systems_f1 = 2 * (self.systems_precision * self.systems_recall) / (self.systems_precision + self.systems_recall)
        
        # Likelihood
        self.likelihood_correct = self.predicted_likelihood == self.expected.likelihood


@dataclass
class AggregateMetrics:
    """Aggregate metrics across all test cases."""
    total_cases: int = 0
    
    # irAE Detection
    irae_accuracy: float = 0.0
    irae_sensitivity: float = 0.0  # True positive rate
    irae_specificity: float = 0.0  # True negative rate
    
    # Severity
    severity_exact_accuracy: float = 0.0
    severity_tolerance_accuracy: float = 0.0
    
    # Urgency
    urgency_accuracy: float = 0.0
    urgency_safety_accuracy: float = 0.0  # Not under-triaging
    
    # Organ Systems
    systems_mean_precision: float = 0.0
    systems_mean_recall: float = 0.0
    systems_mean_f1: float = 0.0
    
    # Likelihood
    likelihood_accuracy: float = 0.0
    
    # Performance
    mean_inference_time_ms: float = 0.0
    
    # Safety metrics
    under_triage_count: int = 0  # Predicted lower urgency than needed
    over_triage_count: int = 0   # Predicted higher urgency than needed
    missed_high_severity: int = 0  # Missed Grade 3-4 cases
    
    def to_dict(self) -> dict:
        """Convert to dictionary for JSON serialization."""
        return {
            "total_cases": self.total_cases,
            "irae_detection": {
                "accuracy": round(self.irae_accuracy, 3),
                "sensitivity": round(self.irae_sensitivity, 3),
                "specificity": round(self.irae_specificity, 3),
            },
            "severity": {
                "exact_accuracy": round(self.severity_exact_accuracy, 3),
                "within_tolerance_accuracy": round(self.severity_tolerance_accuracy, 3),
            },
            "urgency": {
                "accuracy": round(self.urgency_accuracy, 3),
                "safety_accuracy": round(self.urgency_safety_accuracy, 3),
            },
            "organ_systems": {
                "mean_precision": round(self.systems_mean_precision, 3),
                "mean_recall": round(self.systems_mean_recall, 3),
                "mean_f1": round(self.systems_mean_f1, 3),
            },
            "likelihood_accuracy": round(self.likelihood_accuracy, 3),
            "performance": {
                "mean_inference_time_ms": round(self.mean_inference_time_ms, 2),
            },
            "safety": {
                "under_triage_count": self.under_triage_count,
                "over_triage_count": self.over_triage_count,
                "missed_high_severity": self.missed_high_severity,
            },
        }


def _severity_rank(severity: Severity) -> int:
    """Get numeric rank for severity comparison."""
    return {
        Severity.UNKNOWN: 0,
        Severity.GRADE_1: 1,
        Severity.GRADE_2: 2,
        Severity.GRADE_3: 3,
        Severity.GRADE_4: 4,
    }.get(severity, 0)


def _urgency_rank(urgency: Urgency) -> int:
    """Get numeric rank for urgency comparison."""
    return {
        Urgency.ROUTINE: 1,
        Urgency.SOON: 2,
        Urgency.URGENT: 3,
        Urgency.EMERGENCY: 4,
    }.get(urgency, 1)


def _parse_severity(value: str) -> Severity:
    """Parse severity string to enum."""
    value_lower = value.lower()
    if "4" in value_lower or "life-threatening" in value_lower:
        return Severity.GRADE_4
    elif "3" in value_lower or "severe" in value_lower:
        return Severity.GRADE_3
    elif "2" in value_lower or "moderate" in value_lower:
        return Severity.GRADE_2
    elif "1" in value_lower or "mild" in value_lower:
        return Severity.GRADE_1
    return Severity.UNKNOWN


def _parse_urgency(value: str) -> Urgency:
    """Parse urgency string to enum."""
    value_lower = value.lower()
    if "emergency" in value_lower or "üî¥" in value_lower:
        return Urgency.EMERGENCY
    elif "urgent" in value_lower or "üü†" in value_lower:
        return Urgency.URGENT
    elif "soon" in value_lower or "üü°" in value_lower:
        return Urgency.SOON
    return Urgency.ROUTINE


def _parse_likelihood(value: str) -> Likelihood:
    """Parse likelihood string to enum."""
    value_lower = value.lower()
    if "highly likely" in value_lower or "high" in value_lower:
        return Likelihood.HIGHLY_LIKELY
    elif "possible" in value_lower:
        return Likelihood.POSSIBLE
    elif "unlikely" in value_lower:
        return Likelihood.UNLIKELY
    return Likelihood.UNCERTAIN


def _parse_systems(systems: List[str]) -> List[OrganSystem]:
    """Parse organ system strings to enums."""
    result = []
    system_map = {
        "gastrointestinal": OrganSystem.GASTROINTESTINAL,
        "gi": OrganSystem.GASTROINTESTINAL,
        "hepatic": OrganSystem.HEPATIC,
        "liver": OrganSystem.HEPATIC,
        "pulmonary": OrganSystem.PULMONARY,
        "lung": OrganSystem.PULMONARY,
        "endocrine": OrganSystem.ENDOCRINE,
        "cardiac": OrganSystem.CARDIAC,
        "skin": OrganSystem.SKIN,
        "dermatologic": OrganSystem.SKIN,
        "neurologic": OrganSystem.NEUROLOGIC,
        "neuro": OrganSystem.NEUROLOGIC,
        "renal": OrganSystem.RENAL,
        "kidney": OrganSystem.RENAL,
        "hematologic": OrganSystem.HEMATOLOGIC,
    }
    for s in systems:
        key = s.lower().strip()
        if key in system_map:
            result.append(system_map[key])
    return result


class EvaluationFramework:
    """
    Comprehensive evaluation framework for irAE detection.
    
    Usage:
        evaluator = EvaluationFramework(use_llm=True)
        metrics = evaluator.evaluate_all(test_cases)
        print(metrics.to_dict())
    """
    
    def __init__(self, use_llm: bool = True):
        """Initialize with assessment engine."""
        self.engine = IRAEAssessmentEngine(use_llm=use_llm)
        self.results: List[EvaluationResult] = []
    
    async def evaluate_case(
        self, 
        patient_data: PatientData, 
        expected: ExpectedOutcome,
        case_id: str = None
    ) -> EvaluationResult:
        """
        Evaluate a single test case.
        
        Args:
            patient_data: Input patient data
            expected: Ground truth outcome
            case_id: Optional identifier
            
        Returns:
            EvaluationResult with all metrics
        """
        import time
        
        start_time = time.time()
        assessment = await self.engine.assess(patient_data)
        inference_time = (time.time() - start_time) * 1000  # Convert to ms
        
        # Extract predictions
        predicted_systems = [
            f.system for f in assessment.affected_systems if f.detected
        ]
        
        result = EvaluationResult(
            case_id=case_id or patient_data.patient_id or "unknown",
            predicted_irae=assessment.irae_detected,
            predicted_severity=assessment.overall_severity,
            predicted_urgency=assessment.urgency,
            predicted_systems=predicted_systems,
            predicted_likelihood=assessment.causality.likelihood,
            expected=expected,
            inference_time_ms=inference_time,
        )
        
        result.calculate_metrics()
        self.results.append(result)
        
        return result
    
    def evaluate_case_sync(
        self, 
        patient_data: PatientData, 
        expected: ExpectedOutcome,
        case_id: str = None
    ) -> EvaluationResult:
        """Synchronous wrapper for evaluate_case."""
        return asyncio.run(self.evaluate_case(patient_data, expected, case_id))
    
    def calculate_aggregate_metrics(self) -> AggregateMetrics:
        """Calculate aggregate metrics across all evaluated cases."""
        if not self.results:
            return AggregateMetrics()
        
        metrics = AggregateMetrics()
        metrics.total_cases = len(self.results)
        
        # Counters for sensitivity/specificity
        true_positives = 0
        true_negatives = 0
        false_positives = 0
        false_negatives = 0
        
        # Accumulators
        urgency_safety_correct = 0
        
        for r in self.results:
            # irAE detection confusion matrix
            if r.expected.irae_detected and r.predicted_irae:
                true_positives += 1
            elif not r.expected.irae_detected and not r.predicted_irae:
                true_negatives += 1
            elif not r.expected.irae_detected and r.predicted_irae:
                false_positives += 1
            else:
                false_negatives += 1
            
            # Urgency safety (not under-triaging)
            pred_rank = _urgency_rank(r.predicted_urgency)
            exp_rank = _urgency_rank(r.expected.urgency)
            if pred_rank < exp_rank:
                metrics.under_triage_count += 1
            elif pred_rank > exp_rank:
                metrics.over_triage_count += 1
            if pred_rank >= exp_rank:
                urgency_safety_correct += 1
            
            # High severity misses
            if _severity_rank(r.expected.severity) >= 3 and _severity_rank(r.predicted_severity) < 3:
                metrics.missed_high_severity += 1
        
        # Calculate rates
        metrics.irae_accuracy = sum(r.irae_correct for r in self.results) / metrics.total_cases
        
        if true_positives + false_negatives > 0:
            metrics.irae_sensitivity = true_positives / (true_positives + false_negatives)
        if true_negatives + false_positives > 0:
            metrics.irae_specificity = true_negatives / (true_negatives + false_positives)
        
        metrics.severity_exact_accuracy = sum(r.severity_exact for r in self.results) / metrics.total_cases
        metrics.severity_tolerance_accuracy = sum(r.severity_within_tolerance for r in self.results) / metrics.total_cases
        
        metrics.urgency_accuracy = sum(r.urgency_correct for r in self.results) / metrics.total_cases
        metrics.urgency_safety_accuracy = urgency_safety_correct / metrics.total_cases
        
        metrics.systems_mean_precision = sum(r.systems_precision for r in self.results) / metrics.total_cases
        metrics.systems_mean_recall = sum(r.systems_recall for r in self.results) / metrics.total_cases
        metrics.systems_mean_f1 = sum(r.systems_f1 for r in self.results) / metrics.total_cases
        
        metrics.likelihood_accuracy = sum(r.likelihood_correct for r in self.results) / metrics.total_cases
        
        metrics.mean_inference_time_ms = sum(r.inference_time_ms for r in self.results) / metrics.total_cases
        
        return metrics
    
    def print_report(self):
        """Print a formatted evaluation report."""
        metrics = self.calculate_aggregate_metrics()
        
        print("\n" + "="*60)
        print("irAE DETECTION SYSTEM - EVALUATION REPORT")
        print("="*60)
        print(f"Total Test Cases: {metrics.total_cases}")
        print()
        
        print("üìä irAE DETECTION")
        print(f"  Accuracy: {metrics.irae_accuracy:.1%}")
        print(f"  Sensitivity (TPR): {metrics.irae_sensitivity:.1%}")
        print(f"  Specificity (TNR): {metrics.irae_specificity:.1%}")
        print()
        
        print("üìä SEVERITY GRADING")
        print(f"  Exact Match: {metrics.severity_exact_accuracy:.1%}")
        print(f"  Within ¬±1 Grade: {metrics.severity_tolerance_accuracy:.1%}")
        print()
        
        print("üìä URGENCY TRIAGE")
        print(f"  Accuracy: {metrics.urgency_accuracy:.1%}")
        print(f"  Safety (no under-triage): {metrics.urgency_safety_accuracy:.1%}")
        print()
        
        print("üìä ORGAN SYSTEM DETECTION")
        print(f"  Precision: {metrics.systems_mean_precision:.1%}")
        print(f"  Recall: {metrics.systems_mean_recall:.1%}")
        print(f"  F1 Score: {metrics.systems_mean_f1:.1%}")
        print()
        
        print("‚ö†Ô∏è SAFETY METRICS")
        print(f"  Under-triage Cases: {metrics.under_triage_count}")
        print(f"  Missed High Severity (Grade 3-4): {metrics.missed_high_severity}")
        print()
        
        print(f"‚è±Ô∏è Mean Inference Time: {metrics.mean_inference_time_ms:.0f}ms")
        print("="*60)
        
        # Pass/Fail summary
        passed = (
            metrics.severity_tolerance_accuracy >= 0.80 and
            metrics.urgency_safety_accuracy >= 0.90 and
            metrics.irae_sensitivity >= 0.90 and
            metrics.missed_high_severity == 0
        )
        
        if passed:
            print("‚úÖ EVALUATION PASSED - System meets production criteria")
        else:
            print("‚ùå EVALUATION FAILED - Improvements needed")
            if metrics.severity_tolerance_accuracy < 0.80:
                print(f"   - Severity accuracy {metrics.severity_tolerance_accuracy:.1%} < 80% target")
            if metrics.urgency_safety_accuracy < 0.90:
                print(f"   - Urgency safety {metrics.urgency_safety_accuracy:.1%} < 90% target")
            if metrics.irae_sensitivity < 0.90:
                print(f"   - irAE sensitivity {metrics.irae_sensitivity:.1%} < 90% target")
            if metrics.missed_high_severity > 0:
                print(f"   - Missed {metrics.missed_high_severity} high-severity cases")
        
        print("="*60 + "\n")
        
        return metrics


# Predefined test cases based on sample_cases.py expected outcomes
TEST_CASES = [
    {
        "case_id": "GI-001",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_2,
            urgency=Urgency.SOON,
            affected_systems=[OrganSystem.GASTROINTESTINAL],
            likelihood=Likelihood.POSSIBLE,
        ),
    },
    {
        "case_id": "GI-002",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_3,
            urgency=Urgency.URGENT,
            affected_systems=[OrganSystem.GASTROINTESTINAL],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "HEP-001",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_2,
            urgency=Urgency.SOON,
            affected_systems=[OrganSystem.HEPATIC],
            likelihood=Likelihood.POSSIBLE,
        ),
    },
    {
        "case_id": "HEP-002",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_3,
            urgency=Urgency.URGENT,
            affected_systems=[OrganSystem.HEPATIC],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "LUNG-001",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_2,
            urgency=Urgency.SOON,
            affected_systems=[OrganSystem.PULMONARY],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "LUNG-002",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_3,
            urgency=Urgency.EMERGENCY,
            affected_systems=[OrganSystem.PULMONARY],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "ENDO-001",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_2,
            urgency=Urgency.SOON,
            affected_systems=[OrganSystem.ENDOCRINE],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "ENDO-002",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_3,
            urgency=Urgency.URGENT,
            affected_systems=[OrganSystem.ENDOCRINE],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "CARD-001",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_4,
            urgency=Urgency.EMERGENCY,
            affected_systems=[OrganSystem.CARDIAC],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "NEURO-001",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_2,
            urgency=Urgency.URGENT,  # Neuro always escalated
            affected_systems=[OrganSystem.NEUROLOGIC],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
    {
        "case_id": "NEURO-002",
        "expected": ExpectedOutcome(
            irae_detected=True,
            severity=Severity.GRADE_3,
            urgency=Urgency.EMERGENCY,
            affected_systems=[OrganSystem.NEUROLOGIC],
            likelihood=Likelihood.HIGHLY_LIKELY,
        ),
    },
]


if __name__ == "__main__":
    # Run evaluation without LLM for quick testing
    print("Running evaluation framework test (rule-based only)...")
    evaluator = EvaluationFramework(use_llm=False)
    
    # Note: This requires patient data to be passed - this is just a framework demo
    print("Evaluation framework loaded successfully!")
    print(f"Predefined test cases: {len(TEST_CASES)}")
    print("\nTo run full evaluation, use:")
    print("  evaluator = EvaluationFramework(use_llm=True)")
    print("  result = await evaluator.evaluate_case(patient_data, expected)")
    print("  evaluator.print_report()")
